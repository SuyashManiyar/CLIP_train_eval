{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import dependencies \n",
    "from PIL import Image\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "import glob\n",
    "import os\n",
    "import pandas as pd\n",
    "import json\n",
    "import numpy as np\n",
    "import clip\n",
    "from torch.utils.data import Dataset, DataLoader, BatchSampler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "from matplotlib.pyplot import imshow\n",
    "import nltk, re, string, collections\n",
    "from nltk.util import ngrams\n",
    "import collections\n",
    "import shutil\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Arrange your dataset such that you have two folders \\nA folder of images \\nA folder of jsons with the same name as images but json extension \\nThe structure iof jsons should be as follows \\nThe folder structure should look like this \\nMAIN FOLDER \\n    IMAGES\\n        img1.jpg\\n        img2.jpg\\n    JSONS\\n        img1.json\\n        img2.json\\n        \\n\\neach of the json file should look like this \\n[\\n    [\\n        \"caption\"\\n    ]\\n]\\n'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''Arrange your dataset such that you have two folders \n",
    "A folder of images \n",
    "A folder of jsons with the same name as images but json extension \n",
    "The structure iof jsons should be as follows \n",
    "The folder structure should look like this \n",
    "MAIN FOLDER \n",
    "    IMAGES\n",
    "        img1.jpg\n",
    "        img2.jpg\n",
    "    JSONS\n",
    "        img1.json\n",
    "        img2.json\n",
    "        \n",
    "\n",
    "each of the json file should look like this \n",
    "[\n",
    "    [\n",
    "        \"caption\"\n",
    "    ]\n",
    "]\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function that links images to the captions by making a dictionary \n",
    "def return_processed_data(IMG_ROOT,JSON_ROOT):\n",
    "    global device\n",
    "    for imgs in os.listdir(IMG_ROOT):\n",
    "        \n",
    "        img_path = IMG_ROOT+\"/\"+imgs\n",
    "        \n",
    "        #removing corrupted images \n",
    "        try:\n",
    "\n",
    "            image = Image.open(img_path)\n",
    "\n",
    "        except:\n",
    "            shutil.move(img_path,'/dump/'+imgs)\n",
    "\n",
    "    img_paths = glob.glob(os.path.join(IMG_ROOT, \"*.jpg\"))\n",
    "    new_img_paths=[]\n",
    "    d = {}\n",
    "    for i, img_path in enumerate(img_paths):\n",
    "        name = img_path.split(\"/\")[-1].split(\".\")[0]\n",
    "        try:\n",
    "            with open(os.path.join(JSON_ROOT, name+\".json\"), \"r\") as f:\n",
    "                \n",
    "                captions = json.load(f)\n",
    "                temp = []\n",
    "                for cap in captions:\n",
    "                    # if \"http\" not in (cap[0]+ ' '+cap[1]) and len(cap[0]+ ' '+cap[1]) >= 8 and len(cap[0]+ ' '+cap[1]) <= 72:\n",
    "                    #     temp.append(cap[0]+ ' '+cap[1])\n",
    "                    temp.append(cap)\n",
    "                    \n",
    "                d[img_path] = cap\n",
    "                new_img_paths.append(img_path)\n",
    "                texts_temp = clip.tokenize(cap).to(device)\n",
    "                # print(texts_temp)\n",
    "        except Exception as e:\n",
    "            print(\"Train Data \")\n",
    "            print(img_path,\" \",d[img_path])\n",
    "            d.pop(img_path)\n",
    "            new_img_paths.remove(img_path)\n",
    "            print(e)\n",
    "            continue\n",
    "\n",
    "    print(len(d))\n",
    "    print(len(new_img_paths))\n",
    "\n",
    "    d_new = {k: d[k] for k in new_img_paths}\n",
    "    \n",
    "    return d_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(device)\n",
    "\n",
    "BATCH_SIZE = 128\n",
    "EPOCH = 5\n",
    "\n",
    "IMG_ROOT_train = \"train_folder/images\"\n",
    "JSON_ROOT_train = \"train_folder/queries\"\n",
    "\n",
    "IMG_ROOT_val = \"val_folder/images\"\n",
    "JSON_ROOT_val = \"val_folder/queries\"\n",
    "\n",
    "\n",
    "\n",
    "d_train=return_processed_data(IMG_ROOT_train,JSON_ROOT_train)\n",
    "d_val=return_processed_data(IMG_ROOT_val,JSON_ROOT_val)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data Loading "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "print(device)\n",
    "model, preprocess = clip.load(\"ViT-B/32\", device=device, jit=False)\n",
    "# model.load_state_dict(torch.load(\"/DATA/penamakuri1/Suyash/retrieval/clip/models/synthetic_best_model.pt\"))\n",
    "# model = nn.DataParallel(model)\n",
    "\n",
    "class RetrievalDataset(Dataset):\n",
    "    def __init__(self, data, preprocess):\n",
    "        self.preprocess = preprocess\n",
    "        self.img_paths = []\n",
    "        self.captions = []\n",
    "        for img_path, captions in data.items():\n",
    "            for cap in captions:\n",
    "                self.img_paths.append(img_path)\n",
    "                self.captions.append(cap)\n",
    "        self.processed_cache = {}\n",
    "        for img_path in data:\n",
    "            self.processed_cache[img_path] = self.preprocess(Image.open(img_path))\n",
    "        self.img_paths_set = list(data.keys())\n",
    "        self.path2label = {path: self.img_paths_set.index(path) for path in self.img_paths_set}\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.captions)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.img_paths[idx]\n",
    "        image = self.processed_cache[img_path]\n",
    "        caption = self.captions[idx]\n",
    "        label = self.path2label[img_path]\n",
    "        return image, caption, label\n",
    "\n",
    "train_dataset = RetrievalDataset(d_train, preprocess)\n",
    "val_dataset = RetrievalDataset(d_val, preprocess)\n",
    "\n",
    "\n",
    "class BalancedBatchSampler(BatchSampler):\n",
    "    \"\"\"\n",
    "    BatchSampler - from a MNIST-like dataset, samples n_classes and within these classes samples n_samples.\n",
    "    Returns batches of size n_classes * n_samples\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, labels, n_classes, n_samples):\n",
    "        self.labels = labels\n",
    "        self.labels_set = list(set(self.labels.numpy()))\n",
    "        self.label_to_indices = {label: np.where(self.labels.numpy() == label)[0]\n",
    "                                 for label in self.labels_set}\n",
    "        for l in self.labels_set:\n",
    "            np.random.shuffle(self.label_to_indices[l])\n",
    "        self.used_label_indices_count = {label: 0 for label in self.labels_set}\n",
    "        self.count = 0\n",
    "        self.n_classes = n_classes\n",
    "        self.n_samples = n_samples\n",
    "        self.n_dataset = len(self.labels)\n",
    "        self.batch_size = self.n_samples * self.n_classes\n",
    "\n",
    "    def __iter__(self):\n",
    "        self.count = 0\n",
    "        while self.count + self.batch_size < self.n_dataset:\n",
    "            classes = np.random.choice(self.labels_set, self.n_classes, replace=False)\n",
    "            indices = []\n",
    "            for class_ in classes:\n",
    "                indices.extend(self.label_to_indices[class_][\n",
    "                               self.used_label_indices_count[class_]:self.used_label_indices_count[\n",
    "                                                                         class_] + self.n_samples])\n",
    "                self.used_label_indices_count[class_] += self.n_samples\n",
    "                if self.used_label_indices_count[class_] + self.n_samples > len(self.label_to_indices[class_]):\n",
    "                    np.random.shuffle(self.label_to_indices[class_])\n",
    "                    self.used_label_indices_count[class_] = 0\n",
    "            yield indices\n",
    "            self.count += self.n_classes * self.n_samples\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.n_dataset // self.batch_size\n",
    "\n",
    "train_labels = torch.tensor([item[2] for item in train_dataset])\n",
    "train_sampler = BalancedBatchSampler(train_labels, BATCH_SIZE, 1)\n",
    "train_dataloader = DataLoader(train_dataset, batch_sampler=train_sampler)\n",
    "\n",
    "val_labels = torch.tensor([item[2] for item in val_dataset])\n",
    "val_sampler = BalancedBatchSampler(val_labels, BATCH_SIZE, 1)\n",
    "val_dataloader = DataLoader(val_dataset, batch_sampler=val_sampler)\n",
    "\n",
    "\n",
    "for i, item in enumerate(train_sampler):\n",
    "#     print(item)\n",
    "#     print(len(item))\n",
    "    labels = []\n",
    "    for idx in item:\n",
    "        label = train_dataset[idx][2]\n",
    "        labels.append(label)\n",
    "    break\n",
    "\n",
    "print(len(labels), len(set(labels)))\n",
    "\n",
    "for batch in train_dataloader:\n",
    "    imgs, txts, labels = batch\n",
    "    print(imgs.shape)\n",
    "    print(len(txts))\n",
    "    print(labels)\n",
    "    print(labels.shape)\n",
    "    print(torch.unique(labels).shape)\n",
    "    break\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model Training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_models_to_fp32(model):\n",
    "    for p in model.parameters():\n",
    "        p.data = p.data.float()\n",
    "        p.grad.data = p.grad.data.float()\n",
    "\n",
    "if device == \"cpu\":\n",
    "    model.float()\n",
    "\n",
    "loss_img = nn.CrossEntropyLoss()\n",
    "loss_txt = nn.CrossEntropyLoss()\n",
    "# optimizer = optim.Adam(model.parameters(), lr=5e-5,betas=(0.9,0.98),eps=1e-6,weight_decay=0.2)\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-5)\n",
    "scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, len(train_dataloader)*EPOCH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_val_loss = 1e5\n",
    "best_ep = -1\n",
    "for epoch in range(EPOCH):\n",
    "    print(f\"running epoch {epoch}, best test loss {best_te_loss} after epoch {best_ep}\")\n",
    "    step = 0\n",
    "    tr_loss = 0\n",
    "    model.train()\n",
    "    pbar = tqdm(train_dataloader, leave=False)\n",
    "    for batch in pbar:\n",
    "        step += 1\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        images, texts, _ = batch\n",
    "        images = images.to(device)\n",
    "        texts = clip.tokenize(texts).to(device)\n",
    "#         print(images.shape, texts.shape)\n",
    "        logits_per_image, logits_per_text = model(images, texts)\n",
    "        ground_truth = torch.arange(BATCH_SIZE).to(device)\n",
    "\n",
    "        total_loss = (loss_img(logits_per_image,ground_truth) + loss_txt(logits_per_text,ground_truth))/2\n",
    "        total_loss.backward()\n",
    "        tr_loss += total_loss.item()\n",
    "        if device == \"cpu\":\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "        else:\n",
    "            convert_models_to_fp32(model)\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "            clip.model.convert_weights(model)\n",
    "        pbar.set_description(f\"train batchCE: {total_loss.item()}\", refresh=True)\n",
    "    tr_loss /= step\n",
    "\n",
    "    step = 0\n",
    "    val_loss = 0\n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        val_pbar = tqdm(val_dataloader, leave=False)\n",
    "        for batch in val_pbar:\n",
    "            step += 1\n",
    "            images, texts, _ = batch\n",
    "            images = images.to(device)\n",
    "            texts = clip.tokenize(texts).to(device)\n",
    "            logits_per_image, logits_per_text = model(images, texts)\n",
    "            ground_truth = torch.arange(BATCH_SIZE).to(device)\n",
    "\n",
    "            total_loss = (loss_img(logits_per_image,ground_truth) + loss_txt(logits_per_text,ground_truth))/2\n",
    "            val_loss += total_loss.item()\n",
    "            val_pbar.set_description(f\"test batchCE: {total_loss.item()}\", refresh=True)\n",
    "        val_loss /= step\n",
    "\n",
    "    if val_loss < best_val_loss:\n",
    "        best_te_loss = val_loss\n",
    "        best_ep = epoch\n",
    "        print(best_ep)\n",
    "        torch.save(model.state_dict(), \"PATH_TO_SAVE_MODEL/Best.pt\")\n",
    "    print(f\"epoch {epoch}, tr_loss {tr_loss}, te_loss {val_loss} , best_epoch was {best_ep}, best te loss {best_val_loss}\")\n",
    "torch.save(model.state_dict(), \"PATH_TO_SAVE_MODEL/Last.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the saved best model we need to create both text and images embeddings for out test dataset so that we can evaluate the modle performance and also perform inferencing on the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMG_ROOT = \"test_images\"\n",
    "JSON_ROOT = \"test_queries\"\n",
    "\n",
    "img_paths = glob.glob(os.path.join(IMG_ROOT, \"*.jpg\"))\n",
    "\n",
    "d = {}\n",
    "for i, img_path in enumerate(img_paths):\n",
    "    name = img_path.split(\"/\")[-1].split(\".\")[0]\n",
    "    try:\n",
    "        with open(os.path.join(JSON_ROOT, name+\".json\"), \"r\") as f:\n",
    "            \n",
    "            captions = json.load(f)\n",
    "            temp = []\n",
    "            for cap in captions:\n",
    "                # if \"http\" not in (cap[0]+ ' '+cap[1]) and len(cap[0]+ ' '+cap[1]) >= 8 and len(cap[0]+ ' '+cap[1]) <= 72:\n",
    "                #     temp.append(cap[0]+ ' '+cap[1])\n",
    "                temp.append(cap)\n",
    "            d[img_path] = cap\n",
    "            \n",
    "    except Exception as e:\n",
    "        # print(e)\n",
    "        continue\n",
    "\n",
    "print(len(d))\n",
    "\n",
    "device = \"cpu\"\n",
    "\n",
    "\n",
    "model.load_state_dict(torch.load(\"Best.pt\"))\n",
    "\n",
    "first_item = next(iter(d.items()))\n",
    "\n",
    "# Print the first key-value pair\n",
    "print(first_item)\n",
    "\n",
    "# print(brrrs)\n",
    "embeddings_dictionary_text={}\n",
    "embeddings_dictionary_images={}\n",
    "for i in tqdm(d.keys()):\n",
    "    text = clip.tokenize(d[i][0]).to(device)\n",
    "    print(i)\n",
    "    print(d[i][0])\n",
    "    \n",
    "\n",
    "    image = preprocess(Image.open(i)).unsqueeze(0).to(device)\n",
    "    with torch.no_grad():\n",
    "        image_features = model.encode_image(image)\n",
    "        embeddings_dictionary_images[i]=image_features\n",
    "        text_features = model.encode_text(text)\n",
    "        embeddings_dictionary_text[i]=text_features\n",
    "\n",
    "with open('text_emeddings.pkl', 'wb') as file: \n",
    "      \n",
    "    # A new file will be created \n",
    "    pickle.dump(embeddings_dictionary_text, file) \n",
    "\n",
    "with open('image_embeddings.pkl', 'wb') as file: \n",
    "      \n",
    "    # A new file will be created \n",
    "    pickle.dump(embeddings_dictionary_images, file) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "embeddings are created and saved in pickle file they can be loaded now to calculate Recall@1, Recall@5 and Recall@10 fpr both image-to-text and text-to-image retrieval models "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('image_embeddings.pkl', 'rb') as file: \n",
    "      \n",
    "    # Call load method to deserialze \n",
    "    image_dictionary = pickle.load(file) \n",
    "    \n",
    "with open('text_embeddings.pkl', 'rb') as file: \n",
    "      \n",
    "    # Call load method to deserialze \n",
    "    text_dictionary = pickle.load(file) \n",
    "    \n",
    "image_features1=[]\n",
    "text_features1=[]\n",
    "for k in image_dictionary.keys():\n",
    "    image_features1.append(image_dictionary[k])\n",
    "    text_features1.append(text_dictionary[k])\n",
    "image_features=[]\n",
    "text_features=[]\n",
    "for i in image_features1:\n",
    "    image_features.append(i[0].cpu().numpy())\n",
    "for i in text_features1:\n",
    "    text_features.append(i[0].cpu().numpy())\n",
    "      \n",
    "      \n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "def calculate_similarity(text_features, image_features):\n",
    "    similarity_matrix = cosine_similarity(text_features, image_features)\n",
    "    return similarity_matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def rank_similarities(similarity_matrix):\n",
    "    ranked_indices = np.argsort(-similarity_matrix, axis=1)  # Descending order\n",
    "    return ranked_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recall_at_k(ranked_indices, ground_truth_indices, k):\n",
    "    recalls = []\n",
    "    for i, ground_truth in enumerate(ground_truth_indices):\n",
    "        top_k = ranked_indices[i, :k]\n",
    "        print(\"ground truth \",ground_truth)\n",
    "        recall = np.intersect1d(top_k, ground_truth).size / len(ground_truth)\n",
    "        # print(recall)\n",
    "        \n",
    "        recalls.append(recall)\n",
    "    return np.mean(recalls)\n",
    "\n",
    "def calculate_recall(similarity_matrix, ground_truth_indices_text_to_image, ground_truth_indices_image_to_text, k):\n",
    "    ranked_indices_text_to_image = rank_similarities(similarity_matrix)\n",
    "    ranked_indices_image_to_text = rank_similarities(similarity_matrix.T)\n",
    "    \n",
    "    recall_text_to_image = recall_at_k(ranked_indices_text_to_image, ground_truth_indices_text_to_image, k)\n",
    "    recall_image_to_text = recall_at_k(ranked_indices_image_to_text, ground_truth_indices_image_to_text, k)\n",
    "    \n",
    "    return recall_text_to_image, recall_image_to_text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l=[]\n",
    "for i in range(len(image_features)):\n",
    "    l.append([i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "similarity_matrix = calculate_similarity(text_features, image_features)\n",
    "k = 10  # Example for Recall@10\n",
    "recall_text_to_image, recall_image_to_text = calculate_recall(similarity_matrix,l,l, k)\n",
    "\n",
    "print(f\"Recall@{k} for Text-to-Image: {recall_text_to_image}\")\n",
    "print(f\"Recall@{k} for Image-to-Text: {recall_image_to_text}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "clip",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
